from transformers import TrOCRProcessor, VisionEncoderDecoderModel
from PIL import Image
from tqdm.auto import tqdm
from urllib.request import urlretrieve
from zipfile import ZipFile
import numpy as np
import matplotlib.pyplot as plt
import torch
import os
import glob

device="cpu"
def read_image(image_path):
    image = Image.open(image_path).convert('RGB')
    return image
	
def ocr(image, processor, model):
    #processor image i uygun bir hale getiriyor
    #the model produces encoded output
    pixel_values = processor(image, return_tensors='pt').pixel_values.to(device) #can also be used as tf
    #generated_ids = model.generate(pixel_values)
    generated_ids = model.generate(pixel_values, max_length=1024, num_beams=4) #I used it to get more accurate output but the result did not change
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0] #decode etme kısmı
    #skip soecial_tokens=True
    return generated_text

def eval_new_data(data_path=None, num_samples=4, model=None):
    image_paths = glob.glob(data_path)
    for i, image_path in tqdm(enumerate(image_paths), total=len(image_paths)):
        if i == num_samples:
            break
        image = read_image(image_path)
        text = ocr(image, processor, model)
        plt.figure(figsize=(7, 4))
        plt.imshow(image)
        plt.title(text)
        plt.axis('off')
        plt.show()

'''
#printed text için
processor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')
model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed').to(device)
'''


processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')
model = VisionEncoderDecoderModel.from_pretrained(
    'microsoft/trocr-base-handwritten'
).to(device)
	
eval_new_data(
    data_path='images/*.png',
    num_samples=1,
    model=model
)

''' The TrOCR model is a neural network that cannot process images directly.
Before that, we need to convert the images into a suitable format.
The TrOCR processor first resizes the images to 384x384 resolution.
Then, it converts the image to a normalized tensor format, which is passed to the model for inference
'''
